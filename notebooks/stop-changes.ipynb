{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('gis': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "interpreter": {
   "hash": "7a46b998b638738956c1932bf84805bb003d57ec03ebcde4956dccddef721556"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "\n",
    "This Jupyter Notebook takes in data from a Google Sheet that contains stop change details and their associated high level categories and outputs a JSON file for each line to be used in the MyBus tool.\n",
    "\n",
    "The output file is used by the MyBus tool's results page and contains the Stop-level changes that are displayed there.\n",
    "\n",
    "Run all cells to output files in the format `#-changes.json`\n",
    "\n",
    "As of 5/28/21, this should output data for 141 lines."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "DATA_INPUT_PATH = '../data/input/'\r\n",
    "DATA_OUTPUT_PATH = '../data/stop-changes/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stop Changes\r\n",
    "\r\n",
    "Data was compiled from spreadsheets and slides provided by Service Planners.\r\n",
    "\r\n",
    "As of 5/29/21, there are 4794 rows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\r\n",
    "GOOGLE_SHEET_LINK = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vQC-r5V2B0xw886UtXO5TU1YXPy_HzyBoQXQOjDWD_qwmBBlvn3fyVsa-BHu0FymPM8gk4O_UxMtJsK/pub?output=csv\"\r\n",
    "# stop_changes = pd.read_csv(DATA_INPUT_PATH + 'stop_changes - ALL.csv',\r\n",
    "#     usecols={'line', 'stop_id', 'service_canceled', 'service_changed', 'service_replaced', 'stop_canceled', 'stop_relocated', 'route_changed', 'owl_service_canceled'})\r\n",
    "stop_changes = pd.read_csv(GOOGLE_SHEET_LINK,\r\n",
    "    usecols={'line', 'stop_id', 'service_canceled', 'service_changed', 'service_replaced', 'stop_canceled', 'stop_relocated', 'route_changed', 'owl_service_canceled'})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Set empty stop_ids to 0, cast all to int and then string\r\n",
    "stop_changes.stop_id = stop_changes.stop_id.fillna(0)\r\n",
    "stop_changes.stop_id = stop_changes.stop_id.astype('int')\r\n",
    "stop_changes.stop_id = stop_changes.stop_id.astype('string')\r\n",
    "\r\n",
    "#stop_changes.count()\r\n",
    "# 5/29/21 - 4974 stop changes rows\r\n",
    "\r\n",
    "#stop_changes[stop_changes.stop_id == '0']\r\n",
    "# 5/29/21 - 2 stops with no stop id (== 0)\r\n",
    "\r\n",
    "stop_changes.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   line stop_id service_canceled service_changed service_replaced  \\\n",
       "0   2.0   14805            False           False            False   \n",
       "1   2.0    6424            False           False            False   \n",
       "2   4.0    5972            False           False            False   \n",
       "3   4.0    1070            False           False            False   \n",
       "4   4.0    5978            False           False            False   \n",
       "\n",
       "  stop_canceled stop_relocated route_changed  owl_service_canceled  \n",
       "0          True          False         False                 False  \n",
       "1          True          False         False                 False  \n",
       "2          True          False         False                 False  \n",
       "3          True          False         False                 False  \n",
       "4          True          False         False                 False  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>service_canceled</th>\n",
       "      <th>service_changed</th>\n",
       "      <th>service_replaced</th>\n",
       "      <th>stop_canceled</th>\n",
       "      <th>stop_relocated</th>\n",
       "      <th>route_changed</th>\n",
       "      <th>owl_service_canceled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>14805</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6424</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5972</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1070</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5978</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleanup\n",
    "\n",
    "## Results \n",
    "\n",
    "There are only 2 entries left with no `stop_id`.  They are for Line 234, Borden / Noble, which doesn't exist in the GTFS.\n",
    "\n",
    "These need to be set to 0 in order to cast the column as type `int`.  Then cast the column to type `string` for standard comparisons with other data.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load GTFS Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "lines = pd.read_csv(DATA_INPUT_PATH + 'routes.txt', \r\n",
    "    usecols={'route_id', 'route_short_name'},\r\n",
    "    dtype={'route_id':'string', 'route_short_name':'string'})\r\n",
    "stops = pd.read_csv(DATA_INPUT_PATH + 'stops.txt',\r\n",
    "    usecols=['stop_id','stop_name','stop_lat','stop_lon'],\r\n",
    "    dtype={'stop_id':'string','stop_name':'string','stop_lat':'float64','stop_lon':'float64'})\r\n",
    "stop_times = pd.read_csv(DATA_INPUT_PATH + 'stop_times.txt',\r\n",
    "    usecols=['trip_id','stop_id','stop_sequence','stop_headsign'],\r\n",
    "    dtype={'trip_id':'string','stop_id':'string','stop_sequence':'int64','stop_headsign':'string'})"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/input/stops.txt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1cfdc0cf5522>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'route_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'route_short_name'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     dtype={'route_id':'string', 'route_short_name':'string'})\n\u001b[1;32m----> 4\u001b[1;33m stops = pd.read_csv(DATA_INPUT_PATH + 'stops.txt',\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stop_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'stop_name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'stop_lat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'stop_lon'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     dtype={'stop_id':'string','stop_name':'string','stop_lat':'float64','stop_lon':'float64'})\n",
      "\u001b[1;32m~\\.conda\\envs\\gis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1043\u001b[0m             )\n\u001b[0;32m   1044\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1045\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1863\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1864\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gis\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m         \"\"\"\n\u001b[1;32m-> 1357\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1358\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gis\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/input/stops.txt'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create List of All Lines used in MyBus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "# Add route_short_name values for the 901 (Orange Line) and 910/950 (Silver Line)\r\n",
    "lines.loc[lines[\"route_id\"] == '910-13139', 'route_short_name'] = '910/950'\r\n",
    "lines.loc[lines[\"route_id\"] == '901-13139', 'route_short_name'] = '901'\r\n",
    "\r\n",
    "# Line 16/17 is listed as only '16' in the GTFS data even though headsigns show 17.\r\n",
    "# Add line 17 back in.\r\n",
    "lines.loc[lines[\"route_id\"] == '16-13139', 'route_short_name'] = '16/17'\r\n",
    "\r\n",
    "# Add back in lines: 177, 244, 489, 788.\r\n",
    "lines.loc[len(lines.index)] = ['177', '177'] # no stops\r\n",
    "lines.loc[len(lines.index)] = ['244', '244'] # has stops\r\n",
    "lines.loc[len(lines.index)] = ['489', '489'] # has stops\r\n",
    "lines.loc[len(lines.index)] = ['788', '788'] # no stops\r\n",
    "\r\n",
    "# Remove the entries for the Dodger Express and L Line (Gold) Shuttle\r\n",
    "lines = lines.loc[~lines[\"route_id\"].isin(['DSE-HG'])]\r\n",
    "lines = lines.loc[~lines[\"route_id\"].isin(['DSE-US'])]\r\n",
    "lines = lines.loc[~lines[\"route_id\"].isin(['854-13139'])]\r\n",
    "\r\n",
    "# Separate out the sister lines.\r\n",
    "lines_array = lines.loc[lines['route_short_name'].str.contains('/'), 'route_short_name'].values\r\n",
    "\r\n",
    "for i, l in enumerate(lines_array):\r\n",
    "    id = lines.loc[lines['route_short_name'] == l]['route_id'].values[0]\r\n",
    "    slash = l.find('/')\r\n",
    "    line1 = l[:slash]\r\n",
    "    line2 = l[slash+1:]\r\n",
    "    \r\n",
    "    lines = lines.loc[~lines[\"route_id\"].isin([id])]\r\n",
    "    newlines = pd.DataFrame([[id, line1], [id, line2]], columns=['route_id', 'route_short_name'])\r\n",
    "    lines = lines.append(newlines, ignore_index=True)\r\n",
    "\r\n",
    "# cast route_short_name to int32 so that we can sort by their integer value\r\n",
    "lines = lines.astype({'route_short_name': 'int32'}).sort_values('route_short_name')\r\n",
    "#lines.head(10)\r\n",
    "\r\n",
    "lines_array = lines.loc[:, 'route_short_name'].values\r\n",
    "#print(str(len(lines_array)) + ' lines')\r\n",
    "#print(lines_array)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Join stops and lines in a simplified view"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "# lines_and_stops = pd.merge(stop_times, stops, how=\"left\", on=\"stop_id\")\r\n",
    "# simple_lines_stops = lines_and_stops[['stop_id','stop_headsign','stop_name']].copy()\r\n",
    "# simple_lines_stops['line'] = np.nan\r\n",
    "# counter = 1\r\n",
    "\r\n",
    "# for line in lines_array:\r\n",
    "#     line_regex = '^' + str(line) + '\\s'\r\n",
    "#     simple_lines_stops.loc[simple_lines_stops['stop_headsign'].str.contains(line_regex), 'line'] = line\r\n",
    "    \r\n",
    "#     counter += 1\r\n",
    "#     if counter % 10 == 0:\r\n",
    "#         print(str(counter) + ' lines processed')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Line 45/127\n",
    "\n",
    "One set of stop changes is listed as being for Line 45/127.  It's unclear whether the data should be shown for one or both, especially since the two lines overlap.  The stops with changes should be compared to the entirety of the stop list for both lines to see which one it matches.\n",
    "\n",
    "The value in the `lines` column for these stop changes was listed as \"45, 127\".  After the below analysis, the `lines` value will be designated as 45.  Two stop_ids listed in the list of changes were not found in the GTFS data - 13271 and 10439."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "# list of all stops with changes for line 45/127\r\n",
    "# changes_45_127 = stop_changes[stop_changes.line == '45, 127']\r\n",
    "# changes_45_127 = changes_45_127[['stop_id']].copy()\r\n",
    "\r\n",
    "# list of all stops for line 45\r\n",
    "# 32,340 total\r\n",
    "# 145 after dropping duplicate stop_ids\r\n",
    "# stops_45 = simple_lines_stops[simple_lines_stops.line == 45].drop_duplicates('stop_id')\r\n",
    "# stops_45.head()\r\n",
    "\r\n",
    "# list of all stops for line 127\r\n",
    "# stops_127 = simple_lines_stops[simple_lines_stops.line == 127].drop_duplicates('stop_id')\r\n",
    "# stops_127.head()\r\n",
    "\r\n",
    "# merged_45 = pd.merge(changes_45_127, stops_45, how='left', on='stop_id', indicator=True)\r\n",
    "\r\n",
    "# merged_45[merged_45._merge != 'both']\r\n",
    "\r\n",
    "# only 2 results left-only, thus the stop changes match line 45.\r\n",
    "# the 2 stops that are not part of line 45's stops: 13271, 10439\r\n",
    "\r\n",
    "# these stop_ids don't even exist at all in the GTFS\r\n",
    "#simple_lines_stops[simple_lines_stops.stop_id == '13271']\r\n",
    "#simple_lines_stops[simple_lines_stops.stop_id == '10439']\r\n",
    "\r\n",
    "# example of a stop_id that does exist\r\n",
    "#simple_lines_stops[simple_lines_stops.stop_id == '2323']\r\n",
    "\r\n",
    "# merged_127 = pd.merge(changes_45_127, stops_127, how='left', on='stop_id', indicator=True)\r\n",
    "# merged_127[merged_127._merge != 'both'].count()\r\n",
    "\r\n",
    "# 57 results left-only, thus the stop changes do not match line 127."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Line 51/52 - 127\n",
    "\n",
    "One set of stop changes is listed as being for Line 51/52-127.  It's unclear whether what line(s) these changes should apply to.\n",
    "\n",
    "Based on the analysis below, the stop changes data will be duplicated and applied to both lines 51 and 52 since it affects both lines' stops that overlap."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "# changes_51_52_127 = stop_changes[stop_changes.line == '51-52, 127']\r\n",
    "# changes_51_52_127 = changes_51_52_127[['stop_id', 'stop_name']].copy()\r\n",
    "\r\n",
    "# stops_51 = simple_lines_stops[simple_lines_stops.line == 51].drop_duplicates('stop_id')\r\n",
    "\r\n",
    "# merged_51 = pd.merge(changes_51_52_127, stops_51, how='left', on='stop_id', indicator=True)\r\n",
    "# merged_51[merged_51._merge != 'both']\r\n",
    "\r\n",
    "# stops_52 = simple_lines_stops[simple_lines_stops.line == 52].drop_duplicates('stop_id')\r\n",
    "\r\n",
    "# merged_52 = pd.merge(changes_51_52_127, stops_52, how='left', on='stop_id', indicator=True)\r\n",
    "# merged_52[merged_52._merge != 'both']\r\n",
    "\r\n",
    "# only one of the stops with changes does not show up for line 52\r\n",
    "# it has to be a stop for line 51 only\r\n",
    "\r\n",
    "# Merge other direction\r\n",
    "# merged_51_right = pd.merge(stops_51, changes_51_52_127, how='left', on='stop_id', indicator=True)\r\n",
    "# merged_51_right[merged_51_right._merge != 'both']\r\n",
    "\r\n",
    "# Merge other direction\r\n",
    "# merged_52_right = pd.merge(stops_52, changes_51_52_127, how='left', on='stop_id', indicator=True)\r\n",
    "# merged_52_right[merged_52_right._merge != 'both']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Line 757\n",
    "\n",
    "Line 757 did not have stops listed in the changes spreadsheet.  \n",
    "\n",
    "Data provided:\n",
    "* 757\tAll Stops Except Adams, Washington, and NB at King - Service Replaced (38)\n",
    "* 757\tAdams - Service Replaced (2)\n",
    "* 757\tWashington - Service Replaced (2)\n",
    "* 757\tNB at King - Service Replaced (1)\n",
    "\n",
    "Text on signage for the 3 special stops indicates that they have been relocated in addition to their service being replaced.\n",
    "\n",
    "Signage quantities mostly match what is in the GTFS data.  First sign should have 39 copies, not 38.\n",
    "\n",
    "Data manually added to stop_changes spreadsheet:\n",
    "* Service replaced - ALL Stops (44 rows)\n",
    "* Stop Relocated - 3 stops (5 rows)\n",
    "    * Western / Adams (both directions)\n",
    "    * Western / Washington (both directions)\n",
    "    * Western / Martin Luther King Jr (Northbound)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "# stops_757 = simple_lines_stops[simple_lines_stops.line == 757]\n",
    "# stops_757 = stops_757.drop_duplicates('stop_id')\n",
    "## 44 unique stop IDs for line 757\n",
    "\n",
    "# stops_757[stops_757.stop_name.str.contains('Adams')]\n",
    "# Western / Adams - 19147, 19148\n",
    "\n",
    "# stops_757[stops_757.stop_name.str.contains('Washington')]\n",
    "# Western / Washington - 65300052, 65300070\n",
    "\n",
    "# stops_757[stops_757.stop_name.str.contains('King')]\n",
    "# Western / Martin Luther King Jr - 65300050, 11257\n",
    "# But need Northbound ONLY\n",
    "\n",
    "# Get coordinates\n",
    "# lines_and_stops[lines_and_stops.stop_id.isin(['65300050', '11257'])].drop_duplicates(subset=['stop_headsign','stop_id'])\n",
    "\n",
    "# After plotting coordinates, northbound is stop_id 65300050\n",
    "\n",
    "# stops_757[~stops_757.stop_id.isin(['19147', '19148', '65300052', '65300070', '65300050'])]\n",
    "# 44 - 5 = 39 stop_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SFV Stop Consolidation - Lines 92, 94, 164, 165, 224, 234, 240\n",
    "\n",
    "Data for these stops was only provided in the form of signage, on which the stop names were listed.\n",
    "\n",
    "Stop IDs found for 25 stop change entries.\n",
    "\n",
    "NO stop ID found in the GTFS for Line 234, stop Borden / Noble."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "# line_92 = simple_lines_stops[simple_lines_stops.line == 92].drop_duplicates('stop_id')\n",
    "\n",
    "# line_92[line_92.stop_name.str.contains('Bellevue / Douglas')]\n",
    "# 381, 8967\n",
    "\n",
    "# line_92[line_92.stop_name.str.contains('Glendale / Earl')]\n",
    "# 2202, 10743\n",
    "\n",
    "# line_94 = simple_lines_stops[simple_lines_stops.line == 94].drop_duplicates('stop_id')\n",
    "\n",
    "# line_94[line_94.stop_name.str.contains('San Fernando / Thompson')]\n",
    "# 5855, 14231\n",
    "\n",
    "# line_164 = simple_lines_stops[simple_lines_stops.line == 164].drop_duplicates('stop_id')\n",
    "\n",
    "# line_164[line_164.stop_name.str.contains('Victory / Evergreen')]\n",
    "# 8147, 16427\n",
    "\n",
    "# line_164[line_164.stop_name.str.contains('Victory / Fairview')]\n",
    "# 8151, 16431\n",
    "\n",
    "# line_164[line_164.stop_name.str.contains('Victory / Lincoln')]\n",
    "# 8155, 16435\n",
    "\n",
    "# line_224 = simple_lines_stops[simple_lines_stops.line == 224].drop_duplicates('stop_id')\n",
    "\n",
    "# line_224[line_224.stop_name.str.contains('Lankershim / Huston')]\n",
    "# 11308, 2821\n",
    "\n",
    "# line_224[line_224.stop_name.str.contains('Lankershim / Collins')]\n",
    "# 2815, 11303\n",
    "\n",
    "# line_234 = simple_lines_stops[simple_lines_stops.line == 234].drop_duplicates('stop_id')\n",
    "\n",
    "# line_234[line_234.stop_name.str.contains('Borden / Noble')]\n",
    "# doesn't exist in GTFS data\n",
    "\n",
    "# line_234[line_234.stop_name.str.contains('Borden / Beaver')]\n",
    "# 453, 20101\n",
    "\n",
    "# line_234[line_234.stop_name.str.contains('Sayre / Wheeler')]\n",
    "# 14509, 20086\n",
    "\n",
    "# line_234[line_234.stop_name.str.contains('Sayre / Kismet')]\n",
    "# 6127, 140737\n",
    "\n",
    "# line_240 = simple_lines_stops[simple_lines_stops.line == 240].drop_duplicates('stop_id')\n",
    "\n",
    "# line_240[line_240.stop_name.str.contains('Ventura / Dixie Canyon')]\n",
    "# 15358, 7011"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lines 111, 207\n",
    "\n",
    "Stop names provided for these two lines, but no stop IDs.\n",
    "\n",
    "Line 111, stop Florence / Orizaba is called Florence / Western on the other side of the street.\n",
    "\n",
    "The GTFS data lists 3 stops named Florence / Western.  After looking at the coordinates, stop id 1960 is the correct one that is opposite the stop at Florence / Orizaba."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "# line_111 = simple_lines_stops[simple_lines_stops.line == 111].drop_duplicates('stop_id')\n",
    "\n",
    "# line_111[line_111.stop_name.str.contains('Florence / Grand')]\n",
    "# 7643, 15984\n",
    "\n",
    "# line_111[line_111.stop_name.str.contains('Florence / Orizaba')]\n",
    "# 10532\n",
    "\n",
    "# line_111[line_111.stop_name.str.contains('Florence / Western')]\n",
    "# Which stop_id?  7648, 1960, 15989\n",
    "\n",
    "# Get coordinates\n",
    "# lines_and_stops[lines_and_stops.stop_id.isin(['10532', '7648', '1960', '15989'])].drop_duplicates('stop_id')\n",
    "\n",
    "# Florence / Orizaba (10532)\n",
    "# 33.952526,-118.127613,red,square,\"Orizaba\"\n",
    "\n",
    "# Florence / Western (7648)\n",
    "# 33.974447,-118.309231,blue,circle,\"Western 1\"\n",
    "\n",
    "# Florence / Western (1960) <-- Correct location\n",
    "# 33.952424,-118.127992,blue,circle,\"Western 2\"\n",
    "\n",
    "# Florence / Western (15989)\n",
    "# 33.974701,-118.308704,blue,circle,\"Western 3\"\n",
    "\n",
    "# line_207 = simple_lines_stops[simple_lines_stops.line == 207].drop_duplicates('stop_id')\n",
    "\n",
    "# line_207[line_207.stop_name.str.contains('Western / 94')]\n",
    "# 140987, 14135\n",
    "\n",
    "# line_207[line_207.stop_name.str.contains('Western / La Mirada')]\n",
    "# 3989, 142120"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Line 90 - Ternimal 28 / East Lot\n",
    "\n",
    "This was listed twice in the source spreadsheet under different travel directions but only the second instance had a stop id listed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "# line_90 = simple_lines_stops[simple_lines_stops.line == 90].drop_duplicates('stop_id')\n",
    "\n",
    "# line_90[line_90.stop_name.str.contains('Terminal')]\n",
    "# 6535"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results After Cleanup\n",
    "\n",
    "Now there are 4,974 rows of stop changes data.  The only missing stop_ids are for Line 234, Borden / Noble, because it doesn't exist in the GTFS.\n",
    "\n",
    "Questions that came up for further analysis:\n",
    "\n",
    "* What stop signs list additional applicable categories?  Specifically, that the stop has been relocated?  This probably applies primarily to service_changed, service_replaced, or route_changed categories.\n",
    "* Can stop_headsign be used to determine travel direction data for that line's stops?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How many Stops are in stop_changes but not the GTFS data?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# How many stops in stop changes are not in the GTFS data?\n",
    "stops_not_in_gtfs = stop_changes[~stop_changes.stop_id.isin(stops.stop_id)]\n",
    "stops_not_in_gtfs = stops_not_in_gtfs[['line', 'stop_id']]\n",
    "\n",
    "stops_not_in_gtfs = stops_not_in_gtfs.sort_values(['line', 'stop_id'])\n",
    "print(stops_not_in_gtfs.count())\n",
    "\n",
    "stops_not_in_gtfs.to_json(DATA_OUTPUT_PATH + 'stops-not-in-gtfs.json', orient='records')\n",
    "\n",
    "# 73 - 2(stop_id=0) = 71 stops in stop_changes data that don't exist in GTFS\n",
    "# sampling of stop_ids that don't exist in the GTFS\n",
    "# ['672','7929','7906','12827','9851','4716','4963','13559','13271','10439']"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'stop_changes' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3aa90e144628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# How many stops in stop changes are not in the GTFS data?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstops_not_in_gtfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop_changes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mstop_changes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstops_not_in_gtfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstops_not_in_gtfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'line'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stop_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstops_not_in_gtfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstops_not_in_gtfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'line'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stop_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop_changes' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# General Analysis of GTFS Data\n",
    "\n",
    "## Duplicate `stop_id`\n",
    "\n",
    "Not counting the 40 entries with no `stop_id`, there are 505 `stop_id`s that occur at least 2 times.\n",
    "\n",
    "There are 55 `stop_id`s that occur at least 3 times.  `30015` (SYL/SF Metrolink Station) occurs 14 times.\n",
    "\n",
    "## Duplicate `line` & `stop_id` combo\n",
    "\n",
    "After removing for non-existent stop_ids, there are 4506 unique line + stop_id combinations.\n",
    "\n",
    "* 4317 combos occur 1 time, 189 combos occur multiple times\n",
    "* 179 combos occur 2 times\n",
    "* 10 combos occur at least 3 times\n",
    "\n",
    "Within the 10 combos that that have at least 3 duplicate entries with the same `line` and `stop_id`, the line 237, stop_id 25001 combo has entries with service_changed and service_replaced selected.\n",
    "\n",
    "## Merge\n",
    "\n",
    "Join datasets to show the stop change categories for the line-stop combos with duplicates.  The 189 line-stop combos matched with 395 rows from the overall dataset.\n",
    "\n",
    "From this identify where each line-stop combo has different change categories applied.\n",
    "\n",
    "This dataframe was exported to CSV and loaded into Google Sheets for easier analysis.\n",
    "Of those 395 rows, 178 line-stop combos have different categories listed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "# Find duplicate stop_ids\n",
    "#stop_changes['stop_id'].value_counts()\n",
    "#stop_changes['stop_id'].value_counts().loc[lambda x : x>2]\n",
    "\n",
    "# stop_id == 30015 (14 occurences)\n",
    "# SYL/SF Metrolink Station, different lines, directions, and locations\n",
    "#stop_changes.loc[stop_changes['stop_id'] == 30015]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "# Find duplicate line + stop_id combos\n",
    "\n",
    "# exclude non-existent stop_ids\n",
    "# stop_changes_existing_stopids = stop_changes.loc[stop_changes['stop_id'] != 0]\n",
    "\n",
    "# stop_changes_existing_stopids = stop_changes_existing_stopids.groupby(['line', 'stop_id']).size().reset_index(name=\"count\")\n",
    "# stop_changes_existing_stopids = stop_changes_existing_stopids.loc[stop_changes_existing_stopids['count'] > 1]\n",
    "\n",
    "# Output file with the stops that have duplicate rows.\n",
    "# filtered_combos = pd.merge(stop_changes_existing_stopids, stop_changes, how='inner', on=['line', 'stop_id']).sort_values(by=['line', 'stop_id'])\n",
    "# filtered_combos.to_csv(DATA_OUTPUT_PATH + 'stop_changes_duplicates.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Output Stop Changes Data for MyBus\n",
    "\n",
    "Output a file for each line.\n",
    "\n",
    "Create a list of lines by dropping duplicate lines.  Loop through the array and output all stop changes for each line into separate files.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "# Create a list of unique line numbers using the list displayed in MyBus\n",
    "# Read lines.json, which is an output of mybus-data.ipynb\n",
    "# As of 5/28/21 - should contain 141 lines\n",
    "lines_mybus = pd.read_json(DATA_OUTPUT_PATH + 'lines.json')\n",
    "\n",
    "arr_lines_mybus = lines_mybus.loc[:, 'route_short_name'].values\n",
    "len(arr_lines_mybus)\n",
    "# 141 lines\n",
    "\n",
    "# This uses the stop_changes dataset, which doesn't give us line numbers for which there are no stop changes\n",
    "# lines_in_stop_changes = stop_changes['line'].drop_duplicates().reset_index()\n",
    "# lines_in_stop_changes = lines_in_stop_changes.loc[:, 'line'].values\n",
    "\n",
    "# len(lines_in_stop_changes)\n",
    "# 71 lines in stop changes"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "# Loop through line numbers and output a file for each line with all data for that line\n",
    "# As of 5/28/21 should output files for 141 lines\n",
    "i = 1\n",
    "\n",
    "for line in arr_lines_mybus:\n",
    "    subset = stop_changes[stop_changes.line == line]\n",
    "    filename = DATA_OUTPUT_PATH + 'changes/' + str(line) + '-changes.json'\n",
    "    \n",
    "    subset.to_json(filename, orient='records')\n",
    "    \n",
    "    print(str(i) + ': Line ' + filename + ' created')\n",
    "    i += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1: Line data-output/changes/2-changes.json created\n",
      "2: Line data-output/changes/4-changes.json created\n",
      "3: Line data-output/changes/10-changes.json created\n",
      "4: Line data-output/changes/14-changes.json created\n",
      "5: Line data-output/changes/16-changes.json created\n",
      "6: Line data-output/changes/17-changes.json created\n",
      "7: Line data-output/changes/18-changes.json created\n",
      "8: Line data-output/changes/20-changes.json created\n",
      "9: Line data-output/changes/28-changes.json created\n",
      "10: Line data-output/changes/30-changes.json created\n",
      "11: Line data-output/changes/33-changes.json created\n",
      "12: Line data-output/changes/35-changes.json created\n",
      "13: Line data-output/changes/37-changes.json created\n",
      "14: Line data-output/changes/38-changes.json created\n",
      "15: Line data-output/changes/40-changes.json created\n",
      "16: Line data-output/changes/45-changes.json created\n",
      "17: Line data-output/changes/48-changes.json created\n",
      "18: Line data-output/changes/51-changes.json created\n",
      "19: Line data-output/changes/52-changes.json created\n",
      "20: Line data-output/changes/53-changes.json created\n",
      "21: Line data-output/changes/55-changes.json created\n",
      "22: Line data-output/changes/60-changes.json created\n",
      "23: Line data-output/changes/62-changes.json created\n",
      "24: Line data-output/changes/66-changes.json created\n",
      "25: Line data-output/changes/68-changes.json created\n",
      "26: Line data-output/changes/70-changes.json created\n",
      "27: Line data-output/changes/71-changes.json created\n",
      "28: Line data-output/changes/76-changes.json created\n",
      "29: Line data-output/changes/78-changes.json created\n",
      "30: Line data-output/changes/79-changes.json created\n",
      "31: Line data-output/changes/81-changes.json created\n",
      "32: Line data-output/changes/83-changes.json created\n",
      "33: Line data-output/changes/90-changes.json created\n",
      "34: Line data-output/changes/91-changes.json created\n",
      "35: Line data-output/changes/92-changes.json created\n",
      "36: Line data-output/changes/94-changes.json created\n",
      "37: Line data-output/changes/96-changes.json created\n",
      "38: Line data-output/changes/102-changes.json created\n",
      "39: Line data-output/changes/105-changes.json created\n",
      "40: Line data-output/changes/106-changes.json created\n",
      "41: Line data-output/changes/108-changes.json created\n",
      "42: Line data-output/changes/110-changes.json created\n",
      "43: Line data-output/changes/111-changes.json created\n",
      "44: Line data-output/changes/115-changes.json created\n",
      "45: Line data-output/changes/117-changes.json created\n",
      "46: Line data-output/changes/120-changes.json created\n",
      "47: Line data-output/changes/125-changes.json created\n",
      "48: Line data-output/changes/127-changes.json created\n",
      "49: Line data-output/changes/128-changes.json created\n",
      "50: Line data-output/changes/130-changes.json created\n",
      "51: Line data-output/changes/150-changes.json created\n",
      "52: Line data-output/changes/152-changes.json created\n",
      "53: Line data-output/changes/154-changes.json created\n",
      "54: Line data-output/changes/155-changes.json created\n",
      "55: Line data-output/changes/158-changes.json created\n",
      "56: Line data-output/changes/161-changes.json created\n",
      "57: Line data-output/changes/162-changes.json created\n",
      "58: Line data-output/changes/163-changes.json created\n",
      "59: Line data-output/changes/164-changes.json created\n",
      "60: Line data-output/changes/165-changes.json created\n",
      "61: Line data-output/changes/166-changes.json created\n",
      "62: Line data-output/changes/167-changes.json created\n",
      "63: Line data-output/changes/169-changes.json created\n",
      "64: Line data-output/changes/175-changes.json created\n",
      "65: Line data-output/changes/176-changes.json created\n",
      "66: Line data-output/changes/177-changes.json created\n",
      "67: Line data-output/changes/180-changes.json created\n",
      "68: Line data-output/changes/181-changes.json created\n",
      "69: Line data-output/changes/183-changes.json created\n",
      "70: Line data-output/changes/200-changes.json created\n",
      "71: Line data-output/changes/201-changes.json created\n",
      "72: Line data-output/changes/202-changes.json created\n",
      "73: Line data-output/changes/204-changes.json created\n",
      "74: Line data-output/changes/205-changes.json created\n",
      "75: Line data-output/changes/206-changes.json created\n",
      "76: Line data-output/changes/207-changes.json created\n",
      "77: Line data-output/changes/209-changes.json created\n",
      "78: Line data-output/changes/210-changes.json created\n",
      "79: Line data-output/changes/211-changes.json created\n",
      "80: Line data-output/changes/212-changes.json created\n",
      "81: Line data-output/changes/215-changes.json created\n",
      "82: Line data-output/changes/217-changes.json created\n",
      "83: Line data-output/changes/218-changes.json created\n",
      "84: Line data-output/changes/222-changes.json created\n",
      "85: Line data-output/changes/224-changes.json created\n",
      "86: Line data-output/changes/230-changes.json created\n",
      "87: Line data-output/changes/232-changes.json created\n",
      "88: Line data-output/changes/233-changes.json created\n",
      "89: Line data-output/changes/234-changes.json created\n",
      "90: Line data-output/changes/236-changes.json created\n",
      "91: Line data-output/changes/237-changes.json created\n",
      "92: Line data-output/changes/239-changes.json created\n",
      "93: Line data-output/changes/240-changes.json created\n",
      "94: Line data-output/changes/242-changes.json created\n",
      "95: Line data-output/changes/243-changes.json created\n",
      "96: Line data-output/changes/244-changes.json created\n",
      "97: Line data-output/changes/245-changes.json created\n",
      "98: Line data-output/changes/246-changes.json created\n",
      "99: Line data-output/changes/251-changes.json created\n",
      "100: Line data-output/changes/252-changes.json created\n",
      "101: Line data-output/changes/256-changes.json created\n",
      "102: Line data-output/changes/258-changes.json created\n",
      "103: Line data-output/changes/260-changes.json created\n",
      "104: Line data-output/changes/264-changes.json created\n",
      "105: Line data-output/changes/265-changes.json created\n",
      "106: Line data-output/changes/266-changes.json created\n",
      "107: Line data-output/changes/267-changes.json created\n",
      "108: Line data-output/changes/268-changes.json created\n",
      "109: Line data-output/changes/344-changes.json created\n",
      "110: Line data-output/changes/460-changes.json created\n",
      "111: Line data-output/changes/487-changes.json created\n",
      "112: Line data-output/changes/489-changes.json created\n",
      "113: Line data-output/changes/501-changes.json created\n",
      "114: Line data-output/changes/534-changes.json created\n",
      "115: Line data-output/changes/550-changes.json created\n",
      "116: Line data-output/changes/577-changes.json created\n",
      "117: Line data-output/changes/601-changes.json created\n",
      "118: Line data-output/changes/602-changes.json created\n",
      "119: Line data-output/changes/603-changes.json created\n",
      "120: Line data-output/changes/605-changes.json created\n",
      "121: Line data-output/changes/611-changes.json created\n",
      "122: Line data-output/changes/656-changes.json created\n",
      "123: Line data-output/changes/665-changes.json created\n",
      "124: Line data-output/changes/685-changes.json created\n",
      "125: Line data-output/changes/686-changes.json created\n",
      "126: Line data-output/changes/687-changes.json created\n",
      "127: Line data-output/changes/704-changes.json created\n",
      "128: Line data-output/changes/720-changes.json created\n",
      "129: Line data-output/changes/733-changes.json created\n",
      "130: Line data-output/changes/734-changes.json created\n",
      "131: Line data-output/changes/744-changes.json created\n",
      "132: Line data-output/changes/750-changes.json created\n",
      "133: Line data-output/changes/754-changes.json created\n",
      "134: Line data-output/changes/757-changes.json created\n",
      "135: Line data-output/changes/770-changes.json created\n",
      "136: Line data-output/changes/780-changes.json created\n",
      "137: Line data-output/changes/788-changes.json created\n",
      "138: Line data-output/changes/794-changes.json created\n",
      "139: Line data-output/changes/901-changes.json created\n",
      "140: Line data-output/changes/910-changes.json created\n",
      "141: Line data-output/changes/950-changes.json created\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}